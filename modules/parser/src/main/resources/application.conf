kafka {
  # A list of host/port pairs to use for establishing the initial connection to the Kafka cluster.
  bootstrap-servers = "localhost:9092"
  bootstrap-servers = ${?KAFKA_BOOTSTRAP_SERVERS}
}

schema-registry {
  # Comma-separated list of URLs for schema registry instances that can be used to register or look up schemas.
  url = "http://localhost:8081"
  url = ${?SCHEMA_REGISTRY_BASE_URL}
}

kafka-consumer {
  # A unique string that identifies the consumer group this consumer belongs to.
  group-id = "marketplace_parser"
  group-id = ${?PARSER_KAFKA_CONSUMER_GROUP_ID}

  # The topics to which the consumer should subscribe.
  topic = "marketplace_handler-events-ozon_request_handled-version_1"
  topic = ${?PARSER_KAFKA_CONSUMER_TOPIC}

  # The maximum amount of data the server should return for a fetch request.
  fetch-max-bytes = 67108864 # 64MiB
  fetch-max-bytes = ${?PARSER_KAFKA_CONSUMER_FETCH_MAX_BYTES}

  # The maximum amount of data per-partition the server will return.
  max-partition-fetch-bytes = 2097152 # 2MiB
  max-partition-fetch-bytes = ${?PARSER_KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES}

  # The maximum number of records returned in a single call to poll().
  max-poll-records = 8192
  max-poll-records = ${?PARSER_KAFKA_CONSUMER_MAX_POLL_RECORDS}

  # The maximum delay between invocations of poll() when using consumer group management.
  max-poll-interval = 5 minutes
  max-poll-interval = ${?PARSER_KAFKA_CONSUMER_MAX_POLL_INTERVAL}

  # FS2 Kafka. The timeout for offset commits.
  commit-timeout = 120 seconds
  commit-timeout = ${?PARSER_KAFKA_CONSUMER_COMMIT_TIMEOUT}

  # FS2 Kafka. Commits offsets in batches of every `commit-every-n-offsets` offsets or ...
  commit-every-n-offsets = 1024
  commit-every-n-offsets = ${?PARSER_KAFKA_CONSUMER_COMMIT_EVERY_N_OFFSETS}

  # ... time window of length `commit-time-window`, whichever happens first.
  commit-time-window = 1600 milliseconds
  commit-time-window = ${?PARSER_KAFKA_CONSUMER_COMMIT_TIME_WINDOW}

  max-concurrent-per-topic = 4096
  max-concurrent-per-topic = ${?PARSER_KAFKA_CONSUMER_MAX_CONCURRENT_PER_TOPIC}
}

kafka-producer {
  # The topic to which the record should be produced.
  topic {
    results-ozon-seller-list-items = "marketplace_parser-results-ozon_seller_list_items-version_1"
    results-ozon-seller-list-items = ${?PARSER_KAFKA_PRODUCER_TOPIC_RESULTS_OZON_SELLER_LIST_ITEMS}

    results-ozon-category-search-results-v2-items = "marketplace_parser-results-ozon_category_search_results_v2_items-version_3"
    results-ozon-category-search-results-v2-items = ${?PARSER_KAFKA_PRODUCER_TOPIC_RESULTS_OZON_CATEGORY_SEARCH_RESULTS_V2_ITEMS}

    results-ozon-category-sold-out-results-v2-items = "marketplace_parser-results-ozon_category_sold_out_results_v2_items-version_1"
    results-ozon-category-sold-out-results-v2-items = ${?PARSER_KAFKA_PRODUCER_TOPIC_RESULTS_OZON_CATEGORY_SOLD_OUT_ITEMS_V2_ITEMS}
  }

  # The compression type for all data generated by the producer.
  compression-type = "zstd"
  compression-type = ${?PARSER_KAFKA_PRODUCER_COMPRESSION_TYPE}

  # This configuration controls the default batch size in bytes.
  batch-size = 1048576 # 1MiB
  batch-size = ${?PARSER_KAFKA_PRODUCER_BATCH_SIZE}

  # This setting gives the upper bound on the delay for batching.
  linger = 1600 milliseconds
  linger = ${?PARSER_KAFKA_PRODUCER_LINGER}

  # FS2 Kafka. The max number of [[ProducerRecords]] to produce in the same batch when using the produce pipe.
  parallelism = 16384
  parallelism = ${?PARSER_KAFKA_PRODUCER_PARALLELISM}
}
